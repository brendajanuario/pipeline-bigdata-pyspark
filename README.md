
# Bigdata Pipeline - Data Engineering project
The project objective is create the data lakehouse for a B2B company that must store the transactional and analytical data of the business.
The final system must deliver structured, clean and consumable data across the business in order to generate reports and find opportunities.
![image](https://user-images.githubusercontent.com/28696396/204111736-04a85c1f-e88b-41ab-9558-3a2a48676114.png)


# Content
- [B2B Data Lakehouse Architecture](#b2b-data-lakehouse-architecture)
- [Data Stack](#data-stack)
- [Data Modeling](#data-modeling)
- [Data Pipeline](#data-pipeline)
- [Tests](#tests)
- [ETL Pipeline and Data Tracking](#etl-pipeline-and-data-tracking)
- [Marketing Lead Sheet](#marketing-lead-sheet)
- [Infrastructure as Code](#infrastructure-as-code)

# B2B Data Lakehouse Architecture

Architecture used in this project:

<p align="center">
  <img src="/archives/datawarehouse_architecture.png" />
</p>

# Data Stack

The stack used in this project:
- AWS S3 Bucket: as datalake;
- AWS EC2: as computacional resource used by Spark;
- AWS Cloud Formation: to set the environment on DataBricks to use AWS resources;
- Databricks Data Science & Engineering: to create the jobs and develop all tasks;
- Databricks SQL Query Editor: to create the views, queries and Dashboards;
- Spark: to process and manipulate bigdata;
- Delta Lake: to time travel, merging data and processing streaming data.
- Great Expectations: to test the data;
- Terraform: to create the IaC.

# Data modeling

This is a relational data lakehouse, using [Delta Lake](https://docs.delta.io/latest/delta-intro.html).
The e-commerce data is stored in Transactional database while weblog data is stored in Analytical.

<p align="center">
  <img src="https://user-images.githubusercontent.com/28696396/204112062-63e56edb-4581-46a8-bcd6-c60e2c7fe384.png"   width="30%" />
</p>

8 tables were created for this project. The tables that are ready to be consumed by the business are:

- companies
- customers
- products
- orders
- web_logs

Because it is a table that stores a lot of data, the `web_logs` table is partitioned by country. Country is a low cardinality column that is commonly used for queries, which improves performance when queried.

The tables that store the raw data that arrives from `order` requests and the `log` generated by the user's web browsing are:

-web_logs_raw
-orders_raw

  These tables are used as a base to generate the final tables.  

<p align="center">
  <img src="/archives/datamodeling.png"  width="85%" />
</p>

# Data pipeline

There are 4 distinct pipelines created for the project:
- Daily ingestion of customer, companies and products data.
- Daily ingestion of order data.
- Ingestion of streaming weblog data.
- Ingestion and creation of random data used in this project.

#### Daily ingestion of customer, companies and products data:

This pipeline is intended to keep customer, company and product data up to date.
This data is written directly to the data lakehouse (system -> data lakehouse).
To avoid the loss of this data in case of an incident, every time an insertion is made in this base, a backup of this data is also updated within the datalake, in the transactional-backup folder.
Here, the `products` table has a dependency on `companies`. If the pipeline tries to ingest data on a table that has been deleted, it will recreate it with the data from the backup in the datalake.

### Daily ingestion of orders data:

This pipeline is composed of 5 tasks:
* Generate random order data in json format and store in datalake.
* Ingest the data into `orders_raw` into the data lakehouse.
* Test the data ingested in `orders_raw`.
* Make the necessary manipulations in `orders_raw` and store in `orders` table.
* Test the data ingested in `orders`.

If the table is deleted, the data is available in the data lake to be consumed again.
In these two tables, an [autoloader](https://docs.databricks.com/ingestion/auto-loader/index.html) was used for ingestion.
The autoloader is a Databricks tool that needs 3 paths to start the ingestion:
i) schema, 
ii) folder for badly formatted/erroneous records and
iii) checkpoint.

The schema (i) will store the structure of the table, preventing corrupted data from being ingested.
If any data is corrupted during ingestion, it will be recorded in a folder (ii) on the datalake to not discard this information.
The checkpoint tracks the data that has already been ingested, so only new records are ingested.
The data was tested using open sorce [great_expectatios](https://greatexpectations.io/).

### Ingestion of streaming log data.
This pipeline is composed of 3 tasks:
* Generate random logs in text format and store in datalake.
* Ingest the data in web_logs_raw into the data lakehouse.
* Make the necessary manipulations in `web_logs_raw` and storage in `web_logs` table.

The structure of this pipeline is very similar to the previous one (2), however here the pipeline is always ingesting and transforming the log data that arrives every second. With the purpose of work with Bigdata, more than 700 milions rows were created for this table.

### Ingestion and creation of random data used in this project.
This pipeline contains the scripts that were used to randomly generate the data.

All the pipelines were set to retry (up to 3 times) and send an email notifying those responsible in case of failure.

# Tests
For the tests performed on the `orders` table, the open source data quality [Great Expectations](https://greatexpectations.io/) was used. It helps data teams eliminate pipeline debt, through data testing, documentation, and profiling.
The execution results are saved in a history and can be displayed in html.
For this project the tests results are in `great_expectations/index.html`


# ETL Pipeline and Data Tracking

In Databricks, each job generates a history with the information of each execution, and in addition, a panel (archives/Job Run dashboard.html) was also created with the information about the execution of the jobs.

In addition, to track the information, Delta Tables have the time travel, which makes it possible to consult the entire history of changes in a table, informing the operation, time and the user/job that performed the change.

Query:
```
SELECT DESCRIBE 
transactional.customers
```

<p align="center">
  <img src="/archives/datafeedchange.png" />
</p>

# Marketing Lead Sheet

To analyze the requested information and future business opportunities, a panel with dashboards and tabular information was created.
The panel was made using the `SQL Query Editor` from Databricks. The queries were saved in the file `Marketing Lead - Queries, inside the view folder. Although this is only stored as a pdf in the repository (archives/Marketing Lead View), it is possible to observe in the tool the values being updated as the data is ingested.
![image](https://user-images.githubusercontent.com/28696396/204111677-956c827f-dc8a-45af-8945-a9c7056a6e60.png)


# Infrastructure as Code

The Infrastructure as code was created using terraform to generate the structure of the pipelines and cluster settings in an automated way. In order for Terraform to be able to safely find the resources created previously and update them properly, the state file (`.tfstate`) has been configured to store inside the datalake.



